{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "047b6c27-58c8-4494-9c36-74200fa4a6ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Assignment: Structure of Prompt Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a00b4c0-ac1d-45fd-92cf-064a9385de1c",
   "metadata": {},
   "source": [
    "# Part 1: Fundamentals of Prompt Flow\n",
    "\n",
    "## Concept Check (Multiple Choice Questions):\n",
    "\n",
    "### 1. What is the purpose of prompt flow in LLM applications?  \n",
    "A) To design how inputs are structured and processed   \n",
    "B) To train new models  \n",
    "C) To monitor external APIs  \n",
    "D) To evaluate usage patterns  \n",
    "\n",
    "**Correct answer:** A) To design how inputs are structured and processed\n",
    "\n",
    "\n",
    "\n",
    "### 2. Which feature of Azure supports prompt flow testing?  \n",
    "A) Integrated Debugging   \n",
    "B) SQL Query Tools  \n",
    "C) Image Recognition Modules  \n",
    "D) Cloud Storage Optimization  \n",
    "\n",
    "**Correct answer:** A) Integrated Debugging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c745ecb9-0650-4f5d-b22a-8ce0b5320d8f",
   "metadata": {},
   "source": [
    "# Application Task:\n",
    "Describe the steps involved in building an LLM application with prompt flow. Identify a use case (e.g., content generation or customer support chatbot) and outline:\n",
    "\n",
    "The inputs, prompts, and outputs for the application.\n",
    "The integrations or APIs needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60893950-8496-429d-8714-5bc1df8ba78b",
   "metadata": {},
   "source": [
    "### Building an LLM Application with Prompt Flow\n",
    "#### Use Case: Customer Support Chatbot\n",
    "\n",
    "Imagine building a chatbot that can help customers by answering common questions, troubleshooting issues, and guiding them through support processes—all powered by a large language model (LLM). Sounds like a great tool for customer support, right?\n",
    "\n",
    "\n",
    "\n",
    "#### Steps to Build the LLM Application with Prompt Flow\n",
    "\n",
    "#### Step 1: Define the Application Scope\n",
    "Before diving into development, it’s important to define exactly what the chatbot will do. For example:\n",
    "- Will it help customers with product inquiries?\n",
    "- Is it designed to troubleshoot common problems?\n",
    "- Or maybe it’s meant to provide updates on orders?\n",
    "\n",
    "Also, think about the types of questions customers might ask so we can better prepare our chatbot to respond accurately.\n",
    "\n",
    "#### Step 2: Set Up the Development Environment\n",
    "To create this chatbot, we can use **Azure Machine Learning** or **Azure OpenAI** along with **Prompt Flow** to design and test the prompts. You’ll also need to install a few essential SDKs:\n",
    "- `azure-ai-textanalytics`\n",
    "- `openai`\n",
    "- `azureml-sdk`\n",
    "\n",
    "#### Step 3: Define Inputs, Prompts, and Outputs\n",
    "\n",
    "Let’s break down the core components:\n",
    "\n",
    "| **Component**  | **Description** |\n",
    "|----------------|----------------|\n",
    "| **Inputs**     | User queries like \"How do I reset my password?\" |\n",
    "| **Prompts**    | Structured instructions to guide the model, such as \"Given a customer question, provide a clear and concise answer based on the FAQ.\" |\n",
    "| **Outputs**    | The AI-generated response, for example, \"To reset your password, go to the login page and click 'Forgot Password'.\" |\n",
    "\n",
    "#### Step 4: Design the Prompt Flow\n",
    "Now, let's focus on the prompt flow. This is where we can get creative:\n",
    "- Start by creating an **initial prompt** to capture the user's intent.\n",
    "- Use **few-shot examples** in the prompt to help the model understand what type of response we’re expecting.\n",
    "- Make the responses **context-aware** by integrating past conversations (so the chatbot remembers the user’s prior questions).\n",
    "- Don’t forget to **test and refine** the flow using Azure’s **Integrated Debugging** tools to make sure everything runs smoothly.\n",
    "\n",
    "#### Step 5: Integrate with APIs and External Systems\n",
    "\n",
    "Now, it’s time to make the chatbot even smarter by integrating it with various APIs and external systems. For instance:\n",
    "\n",
    "| **Integration**       | **Purpose** |\n",
    "|----------------------|------------|\n",
    "| **Azure OpenAI API** | Generates responses from the LLM |\n",
    "| **Azure Cognitive Search** | Fetches helpful FAQ or knowledge base articles on demand |\n",
    "| **Customer Database API** | Pulls up customer-specific information, like order status |\n",
    "| **Azure Speech-to-Text** (Optional) | Allows the chatbot to support voice-based interactions |\n",
    "\n",
    "#### Step 6: Deploy and Monitor\n",
    "Once everything is in place, you can deploy your chatbot:\n",
    "- Consider deploying it as a **web app** or integrating it into platforms like **Microsoft Teams**, **Slack**, or directly on your **website**.\n",
    "- Use **Azure Monitor** to track the chatbot’s performance, identify areas for improvement, and refine prompts over time.\n",
    "\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "By leveraging **Prompt Flow** in **Azure**, developers can design efficient, optimized prompt interactions for their LLM-based applications. This ensures high-quality, seamless customer interactions, and a chatbot that gets smarter with every conversation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a810c529-04c7-40cc-b6e4-8aacce99c2f2",
   "metadata": {},
   "source": [
    "## Part 2: Building LLM Applications\n",
    "## Case Study Activity:\n",
    "#### Using Azure’s visual editor, design a prototype for a content generation tool that accepts a user topic and generates a blog post draft.\n",
    "\n",
    "#### Explain the components of your prompt flow:\n",
    "#### Input nodes (user topic).\n",
    "#### Model nodes (LLM generating the draft).\n",
    "#### Output nodes (displaying the draft).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f91ab8-a58a-4412-92bf-55fb0d1fd4bc",
   "metadata": {},
   "source": [
    "### Case Study Activity: Content Generation Tool using Azure's Visual Editor\n",
    "\n",
    "#### Prompt Flow Components:\n",
    "\n",
    "1. **Input Nodes (User Topic):**  \n",
    "   The first step is where the user provides a topic for the blog post. This happens through an input node, where the user types in a keyword or phrase, like \"machine learning trends in 2025.\"\n",
    "\n",
    "2. **Model Nodes (LLM Generating the Draft):**  \n",
    "   After the topic is given, the input goes to the model node, where the LLM generates the blog post. The prompt used here could be:  \n",
    "   *\"Write a detailed blog post about [user topic], including an introduction, a main body, and a conclusion.\"*\n",
    "\n",
    "3. **Output Nodes (Displaying the Draft):**  \n",
    "   Once the blog post is generated, the output node displays the draft to the user. This could be in a simple text editor or a downloadable document.\n",
    "\n",
    "\n",
    "#### Write a 200-word reflection on the challenges you faced in designing this flow and how you overcame them using Azure’s tools.\n",
    "#### Reflection on Challenges and Solutions:\n",
    "\n",
    "Designing this content generation tool was interesting, but there were some challenges along the way. One of the main issues was making sure the LLM could generate a well-structured blog post for any topic. It took some testing to find the right prompt that guided the model to write a post with clear sections like an introduction, main body, and conclusion.\n",
    "\n",
    "To solve this, I used **Azure's debugging tools** to test different prompts and quickly see what worked. This helped me adjust the instructions to make the output better. Another challenge was handling different types of topics that users might enter. I used **Azure's Visual Prompt Flow Editor** to organize and tweak the flow, which made it easier to make changes without needing a lot of code.\n",
    "\n",
    "In the end, Azure’s tools helped make the process smoother, allowing me to build a tool that generates clean and readable blog posts from any user input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abf77ea-eece-4f08-845f-6e787c818ef2",
   "metadata": {},
   "source": [
    "## Part 3: Monitoring and Maintaining LLM Applications\n",
    "#### Concept Check (True/False):\n",
    "\n",
    "- Monitoring ensures application performance and helps identify potential issues. (True)\n",
    "- Version control is not necessary for maintaining LLM applications. (False)\n",
    "#### Reflection Activity:\n",
    "#### In 150–200 words, discuss how monitoring metrics like latency and error rates help improve the user experience of an LLM application. Provide examples of tools or strategies that can be used for effective monitoring in Azure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e7c791-4930-4196-9e81-f963d6d31dbc",
   "metadata": {},
   "source": [
    "Monitoring metrics like latency and error rates is super important for making sure users have a smooth experience with an LLM application. **Latency** is basically how long it takes for the system to process a request and give a response. If it takes too long, users might get frustrated and feel like the app is slow. By watching latency, developers can spot any slowdowns and fix them to keep things running smoothly.\n",
    "\n",
    "**Error rates** track how often things go wrong in the app, whether it's bugs, issues with the LLM model, or problems with how everything works together. If the error rate is high, that’s a red flag. Monitoring it helps developers catch problems early and reduce the chances of users getting broken or wrong answers.\n",
    "\n",
    "In Azure, there are tools like Azure Monitor, Application Insights, and Log Analytics that make it easy to track things like latency and error rates. These tools provide real-time monitoring, detailed logs, and insights into performance, helping developers fine-tune the app for a better user experience.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bcbd66-c63f-4b24-8291-6015e911621d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
