{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74bc123-f860-4d52-a8a5-416cf80142be",
   "metadata": {},
   "source": [
    "# Assignment: Cloud-Based Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c6bd5-eb70-4759-92ca-5f5920fe6d3d",
   "metadata": {},
   "source": [
    "### Part 1: Fundamentals of Fine-Tuning\n",
    "**Concept Check (Multiple Choice Questions):**\n",
    "\n",
    "**What is the key benefit of fine-tuning a pre-trained model?**\n",
    "\n",
    "- A) It reduces the need for computational resources\n",
    "- B) It avoids using training data\n",
    "- C) It removes the need for evaluation\n",
    "- D) It simplifies deployment\n",
    "\n",
    "**Correct Answer:** A) It reduces the need for computational resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9cfd0d-acac-401f-abd9-8c4d827c282d",
   "metadata": {},
   "source": [
    "### Which of the following tools optimizes model deployment in Azure?\n",
    "\n",
    "- A) ONNX Runtime \n",
    "- B) TensorBoard\n",
    "- C) Google Sheets\n",
    "- D) SQL Server\n",
    "\n",
    "**Correct Answer:** ONNX Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8365b8fc-8988-4e16-a846-1e09ff897063",
   "metadata": {},
   "source": [
    "### Application Task: Identify Three Potential Tasks for Fine-Tuning\n",
    "#### Identify three potential tasks for which fine-tuning can be applied (e.g., legal document summarization, sentiment analysis, or image captioning).\n",
    "\n",
    "#### For each task:\n",
    "#### Describe the specific pre-trained model you would choose (e.g., GPT, BERT).\n",
    "#### Explain why fine-tuning is beneficial for that task.\n",
    "\n",
    "#### 1. Legal Document Summarization\n",
    "- **Pre-trained Model**: BERT (because it understands complex text well)\n",
    "- **Why Fine-Tuning is Beneficial**: \n",
    "   BERT (Bidirectional Encoder Representations from Transformers) is already trained on a massive amount of general text, but when it comes to understanding legal language, we need it to get more specific. Fine-tuning BERT on a set of legal documents helps it better understand legal terms, jargon, and the structure of legal writing. This means we don't have to train a model from scratch, and BERT will do a better job at summarizing legal documents because it's learned to handle the specific language used in law.\n",
    "\n",
    "#### 2. Sentiment Analysis on Product Reviews\n",
    "- **Pre-trained Model**: RoBERTa (because it's an improved version of BERT with better text comprehension)\n",
    "- **Why Fine-Tuning is Beneficial**: \n",
    "   RoBERTa (Robustly Optimized BERT Pretraining Approach) is really good at understanding how text works in a general sense, but to accurately assess how people feel about products (whether reviews are positive, negative, or neutral), we need to teach it specifically about product reviews. Fine-tuning RoBERTa on a dataset of product reviews helps it understand the type of language people use when discussing products, leading to better predictions about the sentiment behind those reviews.\n",
    "\n",
    "#### 3. Image Captioning\n",
    "- **Pre-trained Model**: CLIP (because it connects images with text effectively)\n",
    "- **Why Fine-Tuning is Beneficial**: \n",
    "   CLIP (Contrastive Language-Image Pre-training) is designed to work with both images and text, which makes it a great tool for generating captions for images. However, to make it really good at describing specific kinds of images (like fashion photos or medical images), we need to fine-tune it on a dataset that includes those kinds of images and their captions. Fine-tuning CLIP helps it focus on understanding and generating captions that are more relevant to the specific domain, which leads to more accurate and useful descriptions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00960896-2a1e-4be3-8ba9-1ed93b7d6855",
   "metadata": {},
   "source": [
    "### Part 2: Implementing Fine-Tuning on Azure\n",
    "\n",
    "#### Case Study Activity:\n",
    "#### Select a pre-trained model from Azure AI Studio’s catalog and fine-tune it for a specific task (e.g., chatbot for customer service or a sentiment analysis tool).\n",
    "\n",
    "**Pre-trained Model Selected**:  \n",
    "I would choose the **BERT model** from Azure AI Studio’s catalog for a **Sentiment Analysis** task. BERT is highly effective for text-based tasks like sentiment analysis, as it has been pre-trained on a large corpus of text and understands the nuances of language.\n",
    "\n",
    "#### Describe the dataset you would use and how you would prepare it.\n",
    "**Dataset and Preparation**:  \n",
    "The dataset I would use is the **Amazon Product Review Dataset**, which contains reviews labeled with sentiments such as positive, neutral, or negative. To prepare the data for fine-tuning, I would clean the text by removing stopwords, HTML tags, and non-relevant characters. Then, I would tokenize the reviews using a BERT-compatible tokenizer to convert the text into tokens. The dataset would be split into training (80%) and testing (20%) sets to evaluate model performance on unseen data. Finally, I would fine-tune the pre-trained BERT model using this dataset, adjusting hyperparameters like the learning rate and batch size for optimal performance.\n",
    "\n",
    "\n",
    "#### Write a 200-word reflection on how you would evaluate the model’s performance after fine-tuning, including metrics you would use and challenges you might face.\n",
    "**Evaluation and Reflection**:  \n",
    "After fine-tuning, I would evaluate the model’s performance using **accuracy** to see how many predictions were correct overall. While accuracy gives a good idea of how the model is performing, it might not be enough, especially if the dataset has an unequal distribution of sentiment categories. So, I would also use the **F1-Score**, which is helpful for balancing **precision** and **recall**. This metric is especially useful when the categories are imbalanced, as it gives a single score that considers both false positives and false negatives. In addition, I would look at a **Confusion Matrix**, which would help me understand where the model is making mistakes, especially which sentiment categories it is confusing the most.\n",
    "\n",
    "A challenge I might face is **data imbalance**, where one sentiment category (like positive reviews) could be more common than others. This could make the model lean too much toward the dominant class. Another issue is **overfitting**, where the model might do well on training data but fail when dealing with new data. To avoid overfitting, I’d use regularization techniques and adjust hyperparameters carefully. I’d also experiment with things like batch sizes, learning rates, and the number of epochs to get the best possible results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec16436-32a9-450f-8f72-4d94d268c1e7",
   "metadata": {},
   "source": [
    "### Part 3: Evaluating and Deploying Models\n",
    "\n",
    "#### Concept Check (True/False):\n",
    "- Fine-tuning eliminates the need for evaluation metrics. (False)\n",
    "- Azure Machine Learning provides tools for real-time monitoring of deployed models. (True)\n",
    "\n",
    "#### Reflection Activity:\n",
    "#### In 150–200 words, discuss the importance of evaluating a fine-tuned model using metrics like F1-Score and cross-validation. Provide examples of potential pitfalls if evaluation is skipped or poorly executed.\n",
    "Evaluating a fine-tuned model is a crucial step before deploying it. Metrics like the **F1-Score** are especially useful when working with imbalanced datasets, like when one sentiment (e.g., positive reviews) is more common than others. The F1-Score ensures that both precision and recall are balanced, so the model does well across all categories (positive, negative, neutral), not just the most common one.  \n",
    "\n",
    "Another technique, **cross-validation**, helps us see if the model can generalize to new data. It works by training the model on different parts of the dataset and testing it on other parts. If we skip these evaluations, we might run into issues like **overfitting**, where the model performs well on the training data but struggles with new data. There could also be **bias** towards one class, which would affect the model’s overall performance.  \n",
    "\n",
    "Without proper evaluation, we might end up with a model that looks great on paper but doesn’t perform well in real-world situations, causing problems for users or businesses depending on it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ed635a-d8a4-4ab9-b32a-037c61ed0e47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
